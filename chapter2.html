<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Глава 2</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <header>
        <div class="center">
            <div class="logo">
                <a href="index.html">На главную</a>
                <!-- <a href="https://drive.google.com/drive/folders/1ZugqcHTbfqaNu64ULXxcGxFxfEIRDbxz">GoogleDrive</a> -->
            </div>
            <div class="menu">
                <a href="contents.html">Содержание</a>
                <a href="introduction.html">Введение</a>
                <a href="chapter1.html">Глава I</a>
                <!-- <a href="chapter2.html">Глава II</a> -->
                <a href="conclusion.html">Заключение</a>
                <a href="appendix.html">Приложения</a>
                <a href="literature.html">Список литературы</a>
                <!-- <a href="https://github.com/BoginskiyVV/webpageDiploma">Репозиторий Github</a>
                <a class="btn-menu" href="#">Ссылка</a> -->
            </div>
        </div>
        <!--Center-->
    </header>
    <section class="main">
        <div class="center">
            <div class="main_cta">
                <h2>Глава II. Практическая часть.</h2>
                <p align="justify">
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.1. Парсер на языке Python с загрузкой данных в SQL формат</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Первичной практической задачей при создании поисково-аналитического
                    механизма было принято создание на языке Python обходчика сайтов по формируемой в процессе
                    исполнения основного файла базе адресов. В качестве первичной базы адресов в сети интернет
                    был обнаружен и скачан текстовый файл с 5 млн. адресами сайтов рунета (5019692) для дальнейшей
                    выемки случайным образом необходимого количества строк адресов для проведения эксперимента.
                    В качестве поэтапного увеличения характеристик эксперимента разработанный обходчик решено
                    тестировать на линейке из 10, 100, 1000 случайных адресов сайтов с использованием библиотеки random.
                    Для простоты первичной фазы эксперимента были выбраны встроенные библиотеки requests и sqlite3
                    языка Python для работы с веб-страницами и SQL-форматом соответственно. Для подсчета времени
                    работы программы использовалась библиотека time.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В ходе разработки программа будет создавать базу данных SiteBase
                    c таблицей SiteTable, в которой будут содержаться текстовые столбцы «www» и «html».
                    Программа открывает существующий в текущей папке файл base.txt и записывает во вновь создаваемый
                    файл adresses.txt необходимое количество строк, которые представляют из себя адреса веб-страниц.
                    Далее создается база данных и с каждой строки файла adresses.txt мы забираем адрес страницы
                    в сети интернет, записываем это значение в ячейку столбца «www», соединяемся со страницей
                    по адресу и записываем ее содержимое в соответствующую ячейку столбца «html» таблицы SiteTable
                    базы данных. В случае ошибки соединения в столбец «html» таблицы SIteTable записывается значение
                    NULL. В процессе исполнения программы ведется подсчет количества скачанных сайтов, количества
                    неудавшихся соединений, а также времени исполнения кода. Когда закончатся строки файла adresses.txt
                    программа печатает строку "адреса закончились", выводит количество скачанных сайтов, количество
                    неудачных соединений и время, затраченное на скачивание страниц.<br>
                    <br>
                    Изображение № 27. Первичное состояние перед исполнением программы<br>
                    <img src="src/chapter2-27.png"><br>
                    <br>
                    Изображение № 28. Код программы<br>
                    <img src="src/chapter2-28.png"><br>
                    <img src="src/chapter2-28-1.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Запускаем код для 10 случайных сайтов, получаем
                    результат:<br>
                    <br>
                    Изображение № 29. Результат обработки 10 случайных адресов<br>
                    <img src="src/chapter2-29.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Фиксируем появление в папке проекта файлов
                    adresses.txt и SiteBase.db:<br>
                    <br>
                    Изображение № 30. Изменения в папке проекта после исполнения программы<br>
                    <img src="src/chapter2-30.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Запускаем программу для выборки из 100 адресов, получаем
                    следующий результат:<br>
                    <br>
                    Изображение № 31. Результат обработки 100 случайных адресов<br>
                    <img src="src/chapter2-31.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Запускаем программу для выборки из 1000 адресов, получаем
                    следующий результат:<br>
                    <br>
                    Изображение № 32. Результат обработки 1000 случайных адресов<br>
                    <img src="src/chapter2-32.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В процессе разработки для верификации возможности дальнейшей
                    работы с полученными данными были разработаны простейшие механизмы проверки. Для
                    начала был написан код на Python, выводящий на печать строки «www» из таблицы
                    SiteTable базы данных:<br>
                    <br>
                    Изображение № 33. Код программы для печати содержимого таблицы БД<br>
                    <img src="src/chapter2-33.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;После запуска кода в терминале происходит печать содержимого
                    ячеек «www» в виде первичного html кода, с сохранением пробелов и отступов, как это
                    было реализовано разработчиками сайтов с соблюдением Document Object Model.<br>
                    <br>
                    Изображение № 34. Печать содержимого БД в терминале<br>
                    <img src="src/chapter2-34.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Далее для подтверждения возможности работы с базой данных
                    в разрезе их анализа была реализована первичная аналитическая программа, позволяющая
                    выявлять частотность слов на всем массиве содержимого столбцов «html» таблицы SiteTable.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;При запуске кода (изображение № 36) мы получаем CSV файл:<br>
                    <br>
                    Изображение № 35. Содержимое сформированного CSV файла<br>
                    <img src="src/chapter2-35.png"><br>
                    <br>
                    Изображение № 35. Код формирования повторений слов в CSV файл<br>
                    <img src="src/chapter2-35-1.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.2. Реализация на Python многопоточной обработки данных
                        для парсинга сайтов с последующим анализом в Jupyter Notebook</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Для отработки навыков парсинга команда писала второй скрипт,
                    основным отличием которого будет многопоточность обработки сайтов, сохранение данных
                    в CSV формате и предобработка html данных.<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Программное обеспечение:<br>
                    • IDE VSCode 1.82.2 (user setup)<br>
                    • python 3.10.11.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Репозиторий с исходным кодом можно будет посмотреть
                    в приложении.<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Этапы написания скрипта:<br>
                    1. Открываем терминал в VSCode "ctrl + ~" :<br>
                    • Создаем виртуальное окружение:<br>
                    <span style="color: #ff6600;">python -m venv venv</span><br>
                    <br>
                    • Активируем виртуальное окружение:<br>
                    <span style="color: #ff6600;">venv/scripts/activate</span><br>
                    <br>
                    • Переходим в папку с виртуальным окружением:<br>
                    <span style="color: #ff6600;">cd ./venv</span><br>
                    <br>
                    В правом нижнем углу VSCode отображается:<br>
                    <span style="color: #ff6600;">venv:venv</span><br>
                    <br>
                    2. Добавляем/обновляем необходимые файлы:<br>
                    • Обновить pip:<br>
                    <span style="color: #ff6600;">python -m pip install --upgrade pip</span><br>
                    <br>
                    3. Формируем файл с зависимостями:<br>
                    <span style="color: #ff6600;">pip freeze > requirements.txt</span><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Все актуальные библиотеки можно посмотреть в файле
                    requirements.txt <br>
                    <br>
                    • Установить зависимости при клонировании репозитория локально:<br>
                    <span style="color: #ff6600;">python -m pip install -r requirements.txt</span><br>
                    <br>
                    4. Загрузить файл с доменом ru<br>
                    Ссылка на сайт с базой зарегистрированных доменов в приложении.<br>
                    Скачиваем zip папку и распаковываем ее в нашу папку с проектом.<br>
                    <br>
                    <br>
                    Изображение № 36. Папка с проектом, содержащая txt файл cо списком url - адресов<br>
                    <img src="src/chapter2-36.png"><br>
                    <br>
                    Изображение № 37. Фрагмент списка с url адресами<br>
                    <img src="src/chapter2-37.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В последующем нам придется дописать https://
                    ко всем строчкам в этом списке, чтобы мы могли зайти на все эти сайты.
                    Список состоит из 5 млн адресов. Наша задача будет состоять в том, чтобы
                    спрасить как можно больше сайтов. Пройтись по всем не получится, так как
                    сайты в данном списке могут быть хорошо защищены от парсинга, а могут
                    и не быть запущены на сервере.<br>
                    <br>
                    5. Создание скрипта в файле spider_parser.py<br>
                    Импортируем библиотеки<br>
                    <br>
                    Изображение № 38. Импорт библиотек<br>
                    <img src="src/chapter2-38.png"><br>
                    <br>
                    • threading - это стандартная библиотека Python, которая предоставляет
                    инструменты для многозадачности. threading позволяет управлять потоками.
                    Потоки позволяют выполнять несколько задач одновременно, что полезно для
                    выполнения параллельных операций. Мы воспользуемся этой библиотекой, чтобы
                    распараллелить потоки обработки парсинга сайтов.<br>
                    • requests - это библиотека Python для выполнения HTTP-запросов. Она
                    облегчает отправку HTTP-запросов к веб-серверам и получение ответов.
                    requests позволяет нам взаимодействовать с веб-ресурсами, получать данные,
                    отправлять данные и многое другое.<br>
                    • csv - это модуль Python для работы с файлами в формате CSV (Comma-Separated Values).
                    Формат CSV используется для хранения табличных данных, где значения разделены запятыми
                    (или другими разделителями). Модуль csv предоставляет функциональность для чтения
                    CSV-файлов, записи данных в CSV-файлы и обработки табличных данных.<br>
                    • re - это модуль Python для работы с регулярными выражениями. Регулярные выражения
                    (или регулярные выражения) используются для поиска и манипуляции текстовой информацией
                    на основе шаблонов. Модуль re позволит нам отфильтровать полученный текст по русским
                    символам. В итоге мы получим чистый текст с русским символами.<br>
                    Создадать файл modif_file.py, прописать функцию, которая будет к каждой строчке url
                    адресов добавлять https:\<br>
                    <br>
                    Изображение № 39. Функция, модифицирующая файл<br>
                    <img src="src/chapter2-39.png"><br>
                    <br>
                    Импортируем функцию url_mod в основной файл spider_parser.py<br>
                    <br>
                    Изображение № 40. Импорт функции<br>
                    <img src="src/chapter2-40.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Чтение файла moifiled_urls.txt. При чтении мы будем
                    записывать данные в формат List и применим метод strip() - это метод строк
                    в Python, который используется для удаления начальных и конечных пробелов
                    (или других указанных символов) из строки. Он создаст новую строчку без
                    лишних пробелов.<br>
                    <br>
                    Изображение № 41. Чтение url - адресов<br>
                    <img src="src/chapter2-41.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Прописать функцию run, которая будет отвечать
                    за парсинг, считывание html кода с первых страниц сайтов. Можно совершенствовать
                    код, до “пробегания” по всем страницам сайтов, но так как это только начало проекта,
                    и мы практикуем основные навыки работы с большими данными, остановимся на самом
                    простом парсинге.<br>
                    <br>
                    Изображение № 42. Class Parser<br>
                    <img src="src/chapter2-42.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;С экземпляром потока и его методами, можно ознакомиться
                    в Приложении №1. Немного модифицировали код для наших целей и получили основную
                    часть кода, которая отвечает за парсинг сайтов. Это многопоточный метод поиска.
                    Для нас он наиболее приемлем, так как большие объемы url адресов обходить
                    получается намного быстрее нежели при обычном обходе.<br>
                    <br>
                    Изображение № 43. Class Parser<br>
                    <img src="src/chapter2-43.png"><br>
                    <br>
                    В конце мы создадим строку заголовка для csv файла.<br>
                    <br>
                    Изображение № 44. Запись CSV-файла<br>
                    <img src="src/chapter2-44.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Проблемы, с которыми мы столкнулись:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;1. Не все сайты из скачанного списка существуют,
                    возможно сервер не работает;<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;2. Не все сайты позволяют обходчику прочитать данные;<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;3. Сохранить данные такого объема на GitHub обычным
                    способом нельзя.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Решение проблем:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;1. Решили продолжать работу по парсингу по тем сайтам,
                    которые существуют. Список сузится, но для начала нашего проекта и этого достаточно.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;2. Решение ошибки 443 находится на стадии разработки;<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;3. Использовали метода работы с большими данными, подключив
                    GIT LFS;<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;4. Указали файлы отслеживания, сохранили изменения
                    и перенесли на Git Hub.<br>
                    <br>
                    Изображение № 45. Git LFS<br>
                    <img src="src/chapter2-45.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Промежуточные результаты:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;1. Написана программа для многопоточного парсинга
                    с извлечением данных с первых страниц сайтов и сохранением их в CSV формате.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;2. Количественные данные:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- Из базы в 5 млн сайтов<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- Обработано 100 000 сайтов<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- Спарсено 11 000 сайтов<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Анализ полученных данных и Jupyter Notebook<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Данные из файла parser.csv представлены двумя столбцами,
                    в первом стоблце url адреса, во втором столбце спарсенные данные в виде текста.
                    Для анализа данных будем использовать Anaconda 1.12.1, python 3.10.13
                    (версии выше не поддерживают библиотеку nltk).<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Загрузку файла осуществили с помощью pandas, подсчет
                    строк провели с помощью мультипроцессинга, чтобы закрепить навык работы с большими
                    данными. Количество строк 11372. Удалили строчки содержащие строчки NaN, осталось
                    9919 строк. Определим функцию, которая очищает текст от некорректных символов.
                    Применили данную функцию к нашему датафрейму, используя два способа: последовательная
                    обработка и мультипроцессинг.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Анализ текста: получим число слов в тексте и число уникальных
                    слов с помощью модуля collections. Результат «Число уникальных слов: 8991», «Сумма
                    частот уникальных слов: 9919».<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Поиск популярных слов производится с помощью библиотеки
                    NLTK. Вывод представлен в виде кортежа, таблицы, графика и облака слов.
                    &nbsp;&nbsp;&nbsp;&nbsp;Стемпинг и лемматизацию провести не получилось, так как
                    получили ошибку «MemoryError: ». Связано с большим объемом данных, попробуем
                    в дальнейшем использовать PySpark.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Частотный анализ слов и словосочетаний провели по тем позициям,
                    которые нас интересуют для дальнейшего исследования.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Ознакомиться с подробным анализом в jupyter notebook можно
                    в приложении.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Результаты:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;1. Провели анализ текста и обработку естественного языка
                    с помощью библиотеки NLTK;<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;2. Освоили методы работы с большими данными
                    в jupyter notebook.<br>
                    <br>
                    Изображение № 46. Схема анализа<br>
                    <img src="src/chapter2-46.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.3 Анализ полученных данных средствами SQL</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Возьмем нашу базу, полученную при помощи формирования данных
                    языком Phyton в CSV – файл и преобразуем для чтения с помощью SQL. При помощи программы
                    EXCEL подготовим данные для таблицы.<br>
                    <br>
                    Изображение № 47. Подготовка данных<br>
                    <img src="src/chapter2-47.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Данные необходимо сохранить в формате :65001: Юникод (UTF-8),
                    указать на формат данных: с разделителем. Вы можете начать импорт с любой строки.
                    Указать заголовки – выбрав галочкой «Мои данные содержат заголовки»
                    (См. Изображение № 48).<br>
                    <br>
                    Изображение № 48.<br>
                    <img src="src/chapter2-48.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Нажав кнопку далее, определим разделители. В нашем случае,
                    это «знак табуляции» и «пробел» (См. Изображение № 49). И кнопку считать
                    последовательные разделители одним. Выставляем ограничение строк до 100.<br>
                    <br>
                    Изображение № 49.<br>
                    <img src="src/chapter2-49.png"><br>
                    <br>
                    При количестве строк величиной 100 получается 15100 – ячеек.<br>
                    <br>
                    Изображение № 50.<br>
                    <img src="src/chapter2-50.png"><br>
                    <br>
                    Сохраняем файл в формате CSV (разделитель- запятая).<br>
                    <br>
                    Изображение № 51.<br>
                    <img src="src/chapter2-51.png"><br>
                    <br>
                    В файле закаченной базы данных 12924 строк. Разбираем данные по файлам поскольку
                    с большим объемом в 500 строк и в 75500 ячеек MySQL не справился.<br>
                    <br>
                    Изображение № 52.<br>
                    <img src="src/chapter2-52.png"><br>
                    <br>
                    Изображение № 53.<br>
                    <img src="src/chapter2-53.png"><br>
                    <br>
                    12924 строки делим по 150 строк в файл и получаем 85 документов для анализа.
                    Заходим в MYSQL в базе данных необходимо выбрать кодировки согласно рисунку 8.<br>
                    <br>
                    Изображение № 54.<br>
                    <img src="src/chapter2-54.png"><br>
                    <br>
                    Нажимаем правой кнопкой мыши на вкладке справа.<br>
                    <br>
                    Изображение № 55.<br>
                    <img src="src/chapter2-55.png"><br>
                    <br>
                    «Table Data Import Wizart» Появляется табло скачивания файла.<br>
                    <br>
                    Изображение № 56.<br>
                    <img src="src/chapter2-56.png"><br>
                    <br>
                    Нажимаем кнопку «NEXT». Появляется окно с символами из нашего файла.<br>
                    Изображение № 57.<br>
                    <img src="src/chapter2-57.png"><br>
                    <br>
                    Нажимаем кнопку «NEXT».<br>
                    <br>
                    Изображение № 58.<br>
                    <img src="src/chapter2-58.png"><br>
                    <br>
                    Таблица появилась в окне «OUTput». Меняем Имя второй колонки, поскольку
                    наличие синтаксических символов мешают программированию.<br>
                    <br>
                    Изображение № 59.<br>
                    <img src="src/chapter2-59.png"><br>
                    <br>
                    Ищем совпадения для построения карты рынка товаров
                    и услуг (Изображения № 60 и 61).<br>
                    <br>
                    Изображение № 60.<br>
                    <img src="src/chapter2-60.png"><br>
                    <br>
                    Изображение № 61.<br>
                    <img src="src/chapter2-61.png"><br>
                    <br>
                    Изображение № 62.<br>
                    <img src="src/chapter2-62.png"><br>
                    <br>
                    С физическими адресами обнаружено 18 объектов.<br>
                    Результат:<br>
                    В результате анализа базы данных 85 файлов обнаружено 1748
                    объектов с товарами и услугами и 1386 физических адресов.<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.4. Анализ и вывод полученных данных с
                        помощью инструментов BI</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В Power BI можно использовать функции и инструменты
                    для поиска строк с повторяющимися значениями в строке. Одной из возможностей
                    является использование Power Query, чтобы выполнить эту задачу. Вот шаги,
                    которые были реализованы:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Открываем Power Query Editor, выбрав соответствующую
                    опцию в меню Power BI. «Текстовы или CSV-файл»:<br>
                    <br>
                    Изображение № 62.<br>
                    <img src="src/chapter2-62-1.png"><br>
                    <br>
                    Загружаем или создаем таблицу, в которой будем искать строки с повторяющимися
                    значениями.<br>
                    <br>
                    Изображение № 63.<br>
                    <img src="src/chapter2-63.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Поскольку нам необходимо найти совпадение в тексте
                    в 2 столбца: URL и content, нажимаем «Извлечение таблицы с использованием
                    примеров.»<br>
                    <br>
                    Изображение № 64.<br>
                    <img src="src/chapter2-64.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Вставляем в заголовок столбец URL, нажимаем на «+»,
                    во второй столбец пишем «content»<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Когда вид столбцов организован, выставляем на
                    основе 200 строк в окне, разделитель – точка с запятой. – извлекаем.<br>
                    <br>
                    Изображение № 65.<br>
                    <img src="src/chapter2-65.png"><br>
                    <br>
                    Появляется таблица с колонками, выделяем столбец, содержащий строковые
                    значения, в которых нужно найти повторы. <br>
                    <br>
                    Изображение № 66.<br>
                    <img src="src/chapter2-66.png"><br>
                    <br>
                    Нажимаем в верхнем меню во вкладке «Преобразование» кнопку «Извлечение»,
                    «Текст после разделителя».<br>
                    <br>
                    Изображение № 67.<br>
                    <img src="src/chapter2-67.png"><br>
                    <br>
                    Вводим текст в нашем случае это «адрес», «товар», «услуги». С формированием
                    одноименных столбцов.<br>
                    <br>
                    Изображение № 68.<br>
                    <img src="src/chapter2-68.png"><br>
                    <br>
                    Далее выводим результат после удаления пустых строк.<br>
                    <br>
                    Изображение № 68.<br>
                    <img src="src/chapter2-68-1.png"><br>
                    <br>
                    Результат:
                    В поле адресов - 831 отдельных и 791 уникальных, товаров - 232 отдельных
                    и 222 уникальных, услуг - 225отдельных и 219 уникальных.<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.5 Разработка аналитического парсер-бота
                        с помощью Python</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Этот раздел представляет разработку простого телеграм-бота,
                    который выполняет анализ веб-сайтов, веденных пользователем. Давайте рассмотрим
                    его разработку и функциональность по шагам:<br>
                    <br>
                    Изображение № 69.<br>
                    <img src="src/chapter2-69.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Импортированные библиотеки:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `telebot`: используется для работы с Telegram API
                    и создания бота.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `requests`: используется для отправки HTTP-запросов
                    к веб-сайту и получения его содержимого.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `BeautifulSoup`: используется для парсинга
                    HTML-контента и извлечения текста.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `Counter`: используется для подсчета количества
                    встречающихся элементов в последовательности.<br>
                    <br>
                    Изображение № 70.<br>
                    <img src="src/chapter2-70.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Установка токена бота и инициализация бота:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `TOKEN`: хранит токен бота.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `bot = telebot.TeleBot(TOKEN)`: Создает экземпляр бота
                    с использованием указанного токена.<br>
                    <br>
                    Изображение № 71.<br>
                    <img src="src/chapter2-71.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Обработчики сообщений:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `handle_start(message)`: Обрабатывает команду `/start`.
                    Отправляет приветственное сообщение и запрашивает адрес веб-сайта.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `handle_website_input(message)`: обрабатывает
                    введенный пользователем адрес веб-сайта. Получает содержимое веб-сайта
                    и предоставляет пользователю опции для анализа.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `handle_analysis_choice(message, html_content)`:
                    Обрабатывает выбор пользователя по типу анализа (топ-10 символов, топ-10 слов,
                    количество символов, пробелов и самый частый символ, полный анализ).<br>
                    <br>
                    Изображение № 72.<br>
                    <img src="src/chapter2-72.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Выбор анализа:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- "1. Топ-10 символов": находит и выводит 10 самых часто
                    встречающихся буквенных символов в тексте.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- "2. Топ-10 слов": находит и выводит 10 самых часто
                    встречающихся слов (более 5 символов) в тексте.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- "3. Количество символов, пробелов, самый частый символ":
                    выводит общее количество символов и пробелов, а также самый часто встречающийся
                    символ и его количество в тексте.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- "4. Полный анализ": Выполнение всех предыдущих пунктов.<br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Запуск бота:<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;- `bot.polling(none_stop=True)`: Запускает бота, который
                    непрерывно опрашивает серверы Telegram на предмет новых сообщений.<br>
                    Изображение № 73.<br>
                    <img src="src/chapter2-73.png"><br>
                    <br>
                    Пример работы бота с сайтом google.com<br>
                    <br>
                    Изображение № 74.<br>
                    <img src="src/chapter2-74.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;<b>3.5. Вывод результатов проекта с помощью создания
                        сайта на HTML/CSS</b><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В целях реализации рекомендаций по написанию дипломной работы,
                    обобщения всех материалов коллективной дипломной работы и визуализации результатов
                    проекта командой было принято решение о создание простой версии веб-страница.
                    В качестве шаблона сайта был выбран один из вариантов простого сайта, а именно шаблон
                    был взят с репозитория github.com (https://github.com/guih58/OnePage).<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Style.css позволяет визуально оформить страницу раскрасить
                    подзаголовки, поменять фон или отформатировать изображение. В каталоге scr/ будут
                    размещены снимки экрана, которые в последствии будут отображены на страницах.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;С учётом множества различных материалов логика сайта была
                    построена таким образом, чтобы пользователь мог просмотреть вложенный контент
                    и переключаться между различными частями дипломной работы, используя меню,
                    расположенное в верхней части экрана.<br>
                    <br>
                    Изображение № 75.<br>
                    <img src="src/chapter2-75.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Для создания и визуализации логического дерева был использован
                    онлайн ресурс https://tree.nathanfriend.io/.<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;В качестве IDE использовалось программное средство
                    Visual Studio Code. Формирование верхнего меню осуществляется в тегах "body header".<br>
                    <br>
                    Изображение № 76.<br>
                    <img src="src/chapter2-76.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Основной текст страниц также размещен в тегах "body header".
                    Для примера взят фрагмент кода страницы contents.html, входящей в состав страниц сайта.<br>
                    <br>
                    Изображение № 77.<br>
                    <img src="src/chapter2-77.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Конечным результатом написания кода является отображение
                    информации в веб браузере.<br>
                    <br>
                    Изображение № 78.<br>
                    <img src="src/chapter2-78.png"><br>
                    <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;Полный код сайта находиться в репозитории github.com,
                    а также сохранён и доступен на drive.google.com.<br>






                </p>

            </div>
        </div>
        </div>
    </section>
</body>

</html>