<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Заключение</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <header>
        <div class="center">
            <div class="logo">
                <a href="index.html">На главную</a>
                <!-- <a href="https://drive.google.com/drive/folders/1ZugqcHTbfqaNu64ULXxcGxFxfEIRDbxz">GoogleDrive</a> -->
            </div>
            <div class="menu">
                <a href="contents.html">Содержание</a>
                <a href="introduction.html">Введение</a>
                <a href="chapter1.html">Глава I</a>
                <a href="chapter2.html">Глава II</a>
                <!-- <a href="conclusion.html">Заключение</a> -->
                <a href="appendix.html">Приложения</a>
                <a href="literature.html">Список литературы</a>
                <!-- <a href="https://github.com/BoginskiyVV/webpageDiploma">Репозиторий Github</a>
                <a class="btn-menu" href="#">Ссылка</a> -->
            </div>
        </div>
        <!--Center-->
    </header>
</header>
<section class="main">
    <div class="center">
        <div class="main_cta">
            <h2>Заключение</h2>
            <p align="justify">
                <br>
                <!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Заключение</b><br><br> -->
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;В представленной коллективной дипломной работе 
                команда проекта ставила перед собой задачи отразить теоретические знания 
                и практический опыт, полученные в результате обучения аналитическим 
                специализациям на платформе GeekBrains. Накопленные знания и опыт были 
                применены для решения задач поиска информации в сети Интернет с последующим 
                хранением и анализом данной информации.<br>
                &nbsp;&nbsp;&nbsp;&nbsp;Основными компонентами технологического стека, 
                представленного GeekBrains в рамках курсов аналитических специальностей, 
                по мнению членов команды, являются языки Python и SQL, позволяющие решать 
                широкий спектр аналитических задач и разрабатывать как небольшие самостоятельные 
                инструменты, так и целые аналитические комплексы. Также отличным инструментарием 
                для анализа и отображения данных является BI, кратко разобранный в теоретической 
                части. Наибольшее внимание в главе, посвященной теории, было уделено различным 
                библиотекам языка Python, которые позволяют собирать информацию в сети Интернет 
                и анализировать полученные данные. Для библиотек requests, BeautifulSoup, Scrapy, 
                Selenium, Pandas, Matplotlib были даны характеристики и описаны особенности 
                использования данных инструментов. Также в главе представлено описание использования 
                различных форматов хранения полученных данных, таких как CSV и JSON, проведено 
                сравнение использования данных инструментов, в том числе относительно использования 
                SQL-форматов.<br>
                &nbsp;&nbsp;&nbsp;&nbsp;В практической части команда попыталась реализовать различные 
                варианты создания парсинговых механизмов и применить несколько подходов для анализа 
                полученных данных. В связи с тем, что проект изначально реализовывался на отдельных 
                локальных машинах, было принято решение ограничиться работой только с текстовой информацией, 
                на заходя в область больших данных. Изначальный файл с адресами для программы-обходчика 
                содержал более 5 млн. адресов сайтов Рунета, которые и стали источником текстовых данных 
                для практической работы. В первом и самом простом варианте реализации парсера, отраженном 
                в разделе 3.1., текстовое (html) содержимое сайтов полностью забиралось на хранение 
                в ячейку таблицы базы данных SQL. Такой способ хорошо работает в случае больших данных 
                и хороших скоростей каналов, когда необходимо собрать исчерпывающую информацию. Среднее 
                время на скачивание html содержимого одного сайта составило более 6 секунд. Это четко 
                показывает, что для решения поисковых задач на больших массивах адресов сайтов метод 
                работает достаточно медленно и лучше оптимизировать задачу уже в процессе обхода, экономя 
                аппаратные ресурсы.<br>
                &nbsp;&nbsp;&nbsp;&nbsp;Второй подход к парсеру сайтов на Python был более осознанным 
                и углубленным, поскольку имело место применение многопоточного подхода и предварительной 
                очистки данных, собираемых для дальнейшего анализа. Также стоит отметить, для хранения 
                данных был использован формат CSV, что позволило увеличить масштаб работы 
                поисково-аналитического механизма с ранее использованного объема обхода в тысячу 
                страниц до десятков тысяч обрабатываемых сайтов. В Приложении №1 представлен Jupyter 
                Notebook проекта, который дает полное понимание о подходах, примененных при решении 
                задач.<br>
                &nbsp;&nbsp;&nbsp;&nbsp;Также команда проекта показала на практике использование SQL 
                и BI инструментов для анализа данных, были подтверждены компетенции разработки 
                чат-ботов на примере создания парсинг-бота, производящего анализ сайта, адрес которого 
                введен пользователем. В качестве инструмента отображения результатов дипломной работы 
                командой был разработан сайт проекта на технологии HTML/CSS с прицелом на будущее 
                создание интерактивного инструмента отображения результатов анализа данных.<br>
                &nbsp;&nbsp;&nbsp;&nbsp;В перспективе проект будет реализован командой уже не на 
                локальных машинах. Планируется разворачивание поисково-аналитического механизма 
                на внешних серверах, что придаст проекту совершенно иные возможности. Также в перспективе 
                будут задействованы механизмы обработки, хранения и анализа больших данных, что потребует 
                от команды вложения в проект денежных средств, что должно быть обусловлено практической 
                обоснованностью проекта в коммерческом плане.
                <br>

            </p>

        </div>
    </div>
    </div>
</section>
</body>

</html>