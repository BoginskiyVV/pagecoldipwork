## 3.1. Разработка поискового механизма на языке Python

Для отработки навыков и изучения инструментов парсинга web-страниц нами был выбран высокоуровневый язык программирования Python. Почему именно он:
- в процессе обучения изучен нами более детально;
- постоянно обновляющиеся библиотеки;
- доступный поиск информации по интересующим нас вопросам.

Мы использовали IDE VSCode 1.82.2 (user setup), python 3.10.11. Создали репозиторий Parser на сайте [GitHub](https://github.com/FiryuzaLapteva/Parser/tree/main) с файлами `Readme.md` и `.gitignore`. В файл `.gitignore` скопировали все файлы, которые git не будет отслеживать, список взяли по ссылке [GitHub .gitignore](https://github.com/FiryuzaLapteva/DZ_Pyt/blob/main/.gitignore).

Работаем в терминале VSCode `ctrl+~`:

Создали виртуальное окружение:
```
python -m venv venv
```

Активировали виртуальное окружение:
```
<venv/scripts/activate>
```
Перешли в папку с виртуальным окружением:
```
<cd ./venv>
```
В правом нижнем углу VSCode отображается:
```
(‘venv’:venv)
```
Создали файл с зависимостями:
```
pip freeze > requirements.txt
```
Все актуальные библиотеки можно посмотреть в файле requirements.txt.
Установить зависимости можно используя команду :
```
 python -m pip install -r requirements.txt.
 ```

Мы будем парсить сайты с доменом ru, в бесплатном доступе их можно скачать с сайта https://purecrawl.com/download/domains . Скачиваем zip папку и распаковываем ее в нашу папку с проектом. 
![папка_с_проектом](src\chapter3-1.png)

Так выглядит список в файле ru.txt. 

![ru.txt](src\chapter3-2.png)

В последующем нам придется дописать https:// ко всем строчкам в этом списке, чтобы мы могли зайти на все эти сайты. Список состоит из 5 млн адресов. Наша задача будет состоять в том, чтобы спрасить как можно больше сайтов. Пройтись по всем не получится, так как сайты в данном списке могут быть хорошо защищены от парсинга, а могут и не быть запущены на сервере.

Прописываем код в файле spider_parser.py

Импортируем библиотеки
```
import threading
import requests
import csv
import re
```
Создадим файл modif_file.py, пропишем в ней функцию, которая будет к каждой строчке url адресов добавлять https:\

![modif_file.py](src\chapter3-3.png)

Импортируем данную функцию в наш основной файл spider_parser.py

![import_modif_file](src\chapter3-4.png)

После того как мы модифицировали файл, нам необходимо его прочитать. При чтении мы будем записывать данные в формат List и применим метод strip() - это метод строк в Python, который используется для удаления начальных и конечных пробелов (или других указанных символов) из строки. Он создаст новую строчку без лишних пробелов.

![read_modif_file](src\chapter3-5.png)

Теперь пропишем основную функцию, которая будет отвечать за парсинг, считывание html кода с первых страниц сайтов. Можно совершенствовать код, до “пробегания” по всем страницам сайтов, но так как это только начало проекта и мы практикуем основные навыки работы с большими данными, и данных для обработки у нас и так достаточно, остановимся на самом простом парсинге. 
Именно здесь нам потребуются все 4 библиотеки, которые мы импортировали в самом начале.
•	threading - это стандартная библиотека Python, которая предоставляет инструменты для многозадачности. threading позволяет управлять потоками. Потоки позволяют выполнять несколько задач одновременно, что полезно для выполнения параллельных операций. Мы воспользуемся этой библиотекой, чтобы распараллелить потоки обработки парсинга сайтов .

•	requests - это библиотека Python для выполнения HTTP-запросов. Она облегчает отправку HTTP-запросов к веб-серверам и получение ответов. requests позволяет нам взаимодействовать с веб-ресурсами, получать данные, отправлять данные и многое другое. 

•	csv - это модуль Python для работы с файлами в формате CSV (Comma-Separated Values). Формат CSV используется для хранения табличных данных, где значения разделены запятыми (или другими разделителями). Модуль csv предоставляет функциональность для чтения CSV-файлов, записи данных в CSV-файлы и обработки табличных данных. 

•	re - это модуль Python для работы с регулярными выражениями. Регулярные выражения (или регулярные выражения) используются для поиска и манипуляции текстовой информацией на основе шаблонов. Модуль re позволит нам отфильтровать полученный текст по русским символам. В итоге мы получим чистый текст с русским символами. 

![parser](src\chapter3-6.png)

Про экземпляр потока и его методы, вы можете ознакомиться по ссылке https://all-python.ru/osnovy/threading.html?ysclid=lmxayom1wc333926930. Немного модифицировали код для наших целей и получили основную часть кода, которая отвечает за парсинг сайтов. Это многопоточный метод поиска. Для нас наиболее приемлен, так как большие объемы url адресов обходить получается намного быстрее нежели при обычном обходе.
На картинке ниже представлена код отвечающий за обработку потоков.

![processor](src\chapter3-7.png)

В конце мы создадим строку заголовка для полученной таблицы.

![table_header](src\chapter3-8.png)

Проблемы, с которыми мы столкнулись:
1.	  Ни все сайты из скаченного списка существуют, возможно сервер не работает;
2.	 Ни все сайты позволяют обходчику прочитать данные;
3.	 Сохранить данные такого объема на GitHub обычным способом нельзя.
Решение проблем:
1.	Решили продолжать работу по парсингу по тем сайтам, которые существую. Список сузится, но для начала нашего проекта и этого достаточно.
2.	Решение ошибки 443 находится на стадии разработки;
3.	Использовали метода работы с большими данными, подключив GIT LFS https://russianblogs.com/article/39983568763/
Указали файлы отслеживания. Далее сохранили изменения и перенесли на Git Hub.

![LFS](src\chapter3-8.png)
